{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CkFTOxMMjvhj"
   },
   "source": [
    "Preparing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEbpgbIpSxub"
   },
   "outputs": [],
   "source": [
    "!/opt/bin/nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e92vyNIB46SC"
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T-0jbMUs1uca"
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import shutil\n",
    "import argparse\n",
    "import zipfile\n",
    "import hashlib\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import IPython.display as display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime, os\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from IPython.display import clear_output\n",
    "import tensorflow_addons as tfa\n",
    "import cv2\n",
    "#from keras.utils import normalize\n",
    "import keras\n",
    "from keras import regularizers\n",
    "\n",
    "# For more information about autotune:\n",
    "# https://www.tensorflow.org/guide/data_performance#prefetching\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "print(f\"Tensorflow ver. {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gWU9mOYkDlB"
   },
   "outputs": [],
   "source": [
    "# important for reproducibility\n",
    "# this allows to generate the same random numbers\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0WUIYKXkKRo"
   },
   "source": [
    "Update Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdFK-G-gkHEn"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nn4o3K96mrDo"
   },
   "outputs": [],
   "source": [
    "root = \"/content/drive/My Drive/BF_TEM_Dataset/\"\n",
    "dataset_path = root + \"images/\"\n",
    "training_data = \"training/\"\n",
    "val_data = \"validation/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_M9fcLmmv9d"
   },
   "source": [
    "Create Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cobtBtpKmysD"
   },
   "outputs": [],
   "source": [
    "# Image size that we are going to use\n",
    "IMG_SIZE = 1024\n",
    "# Our images are gray scale (1 channels)\n",
    "N_CHANNELS = 1\n",
    "# Scene Parsing has 2 classes, ie boundary & non-boundary\n",
    "N_CLASSES = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Op0B7jApm7nf"
   },
   "outputs": [],
   "source": [
    "TRAINSET_SIZE = len(glob(dataset_path + training_data + \"*.jpg\"))\n",
    "print(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n",
    "\n",
    "VALSET_SIZE = len(glob(dataset_path + val_data + \"*.jpg\"))\n",
    "print(f\"The Validation Dataset contains {VALSET_SIZE} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1li0qoSqm_ni"
   },
   "outputs": [],
   "source": [
    "def parse_image(img_path: str) -> dict:\n",
    "    \"\"\"Load an image and its annotation (mask) and returning\n",
    "    a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : str\n",
    "        Image (not the mask) location.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    # For one Image path:\n",
    "    # ...\n",
    "    # Its corresponding annotation path is:\n",
    "    # ...\n",
    "    mask_path = tf.strings.regex_replace(img_path, \"images\", \"annotations\")\n",
    "    mask_path = tf.strings.regex_replace(mask_path, \"jpg\", \"png\")\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    # The masks contain a class index for each pixels\n",
    "    mask = tf.image.decode_png(mask, channels=1)\n",
    "\n",
    "    # The tf.image.decode_png has a problem\n",
    "    # It cannot copy the png pixel value correctly\n",
    "    # The original png file contains pixel=0 or 1\n",
    "    # but after the decode_png, the values pixel=254 or 255\n",
    "    # We need to correct the values back, because the N_CLASS are related to the pixel value range !!!\n",
    "    mask = tf.where(mask == 255, np.dtype('uint8').type(1), mask)\n",
    "    mask = tf.where(mask == 254, np.dtype('uint8').type(0), mask)\n",
    "    # Note that we have to convert the new value (0)\n",
    "    # With the same dtype than the tensor itself\n",
    "\n",
    "    return {'image': image, 'segmentation_mask': mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mtnJ7utnIhM"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.list_files(dataset_path + training_data + \"*.jpg\", seed=SEED)\n",
    "train_dataset = train_dataset.map(parse_image)\n",
    "\n",
    "val_dataset = tf.data.Dataset.list_files(dataset_path + val_data + \"*.jpg\", seed=SEED)\n",
    "val_dataset = val_dataset.map(parse_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "og-Sz7oWnLmk"
   },
   "outputs": [],
   "source": [
    "type(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7miy6X3_nQVA"
   },
   "outputs": [],
   "source": [
    "# Here we are using the decorator @tf.function\n",
    "# if you want to know more about it:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/function\n",
    "\n",
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image, input_mask\n",
    "\n",
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "    \"\"\"Apply some transformations to an input dictionary\n",
    "    containing a train image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    An annotation is a regular  channel image.\n",
    "    If a transformation such as rotation is applied to the image,\n",
    "    the same transformation has to be applied on the annotation also.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    if tf.random.uniform(()) > 0.5:\n",
    "        input_image = tf.image.flip_left_right(input_image)\n",
    "        input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask\n",
    "\n",
    "@tf.function\n",
    "def load_image_test(datapoint: dict) -> tuple:\n",
    "    \"\"\"Normalize and resize a test image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since this is for the test set, we don't need to apply\n",
    "    any data augmentation technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "    return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qGE0fTc0nWRw"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 2\n",
    "\n",
    "# for reference about the BUFFER_SIZE in shuffle:\n",
    "# https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle\n",
    "BUFFER_SIZE = 48\n",
    "\n",
    "dataset = {\"train\": train_dataset, \"val\": val_dataset}\n",
    "\n",
    "# -- Train Dataset --#\n",
    "dataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "dataset['train'] = dataset['train'].repeat()\n",
    "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
    "dataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "#-- Validation Dataset --#\n",
    "dataset['val'] = dataset['val'].map(load_image_test)\n",
    "dataset['val'] = dataset['val'].repeat()\n",
    "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
    "dataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "print(dataset['train'])\n",
    "print(dataset['val'])\n",
    "\n",
    "# how shuffle works: https://stackoverflow.com/a/53517848"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSraSSFTncvZ"
   },
   "source": [
    "Visualizing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vC59Txc8ng6p"
   },
   "outputs": [],
   "source": [
    "def display_sample(display_list):\n",
    "    \"\"\"Show side-by-side an input image,\n",
    "    the ground truth and the prediction.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(18, 18))\n",
    "\n",
    "    title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "    for i in range(len(display_list)):\n",
    "        plt.subplot(1, len(display_list), i+1)\n",
    "        plt.title(title[i])\n",
    "        plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyQx0ta4nmX9"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset['train'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "display_sample([sample_image[0], sample_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j4Qj6Pz5qsh7"
   },
   "outputs": [],
   "source": [
    "sample_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBrik_ykqtxW"
   },
   "outputs": [],
   "source": [
    "array_a=np.array(sample_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VF7LS8lwq1Ql"
   },
   "outputs": [],
   "source": [
    "array_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRtjP5jJq4jN"
   },
   "outputs": [],
   "source": [
    "imghist=array_a.reshape(array_a.size,1)\n",
    "plt.hist(imghist, bins = 50)\n",
    "plt.title(\"histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OyyTq0A3rAa6"
   },
   "outputs": [],
   "source": [
    "array_b=np.array(sample_image)\n",
    "imghist=array_b.reshape(array_b.size,1)\n",
    "plt.hist(imghist, bins = 50)\n",
    "plt.title(\"histogram\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnVMN8ssrMzG"
   },
   "source": [
    "Developing the Model (UNet) Using Keras Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiM-5C5BrKkm"
   },
   "outputs": [],
   "source": [
    "# -- Keras Functional API -- #\n",
    "# -- UNet Implementation -- #\n",
    "# Everything here is from tensorflow.keras.layers\n",
    "# I imported tensorflow.keras.layers * to make it easier to read\n",
    "input_size = (IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
    "\n",
    "# If you want to know more about why we are using `he_normal`:\n",
    "# https://stats.stackexchange.com/questions/319323/whats-the-difference-between-variance-scaling-initializer-and-xavier-initialize/319849#319849\n",
    "# Or the excelent fastai course:\n",
    "# https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb\n",
    "initializer = 'he_normal'\n",
    "\n",
    "\n",
    "# -- Encoder -- #\n",
    "# Block encoder 1\n",
    "inputs = Input(shape=input_size)\n",
    "conv_enc_1 = Conv2D(64, 3, activation='relu', padding='same', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.0001))(inputs)\n",
    "conv_enc_1 = Conv2D(64, 3, activation = 'relu', padding='same', kernel_initializer=initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_enc_1)\n",
    "\n",
    "# Block encoder 2\n",
    "max_pool_enc_2 = MaxPooling2D(pool_size=(2, 2))(conv_enc_1)\n",
    "conv_enc_2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(max_pool_enc_2)\n",
    "conv_enc_2 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_enc_2)\n",
    "\n",
    "# Block  encoder 3\n",
    "max_pool_enc_3 = MaxPooling2D(pool_size=(2, 2))(conv_enc_2)\n",
    "conv_enc_3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(max_pool_enc_3)\n",
    "conv_enc_3 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_enc_3)\n",
    "\n",
    "# Block  encoder 4\n",
    "max_pool_enc_4 = MaxPooling2D(pool_size=(2, 2))(conv_enc_3)\n",
    "conv_enc_4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(max_pool_enc_4)\n",
    "conv_enc_4 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_enc_4)\n",
    "\n",
    "# -- Encoder -- #\n",
    "\n",
    "# ----------- #\n",
    "maxpool = MaxPooling2D(pool_size=(2, 2))(conv_enc_4)\n",
    "conv = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(maxpool)\n",
    "conv = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv)\n",
    "\n",
    "# ----------- #\n",
    "\n",
    "# -- Dencoder -- #\n",
    "# Block decoder 1\n",
    "up_dec_1 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(UpSampling2D(size = (2,2))(conv))\n",
    "merge_dec_1 = concatenate([conv_enc_4, up_dec_1], axis = 3)\n",
    "conv_dec_1 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(merge_dec_1)\n",
    "conv_dec_1 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_dec_1)\n",
    "\n",
    "# Block decoder 2\n",
    "up_dec_2 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(UpSampling2D(size = (2,2))(conv_dec_1))\n",
    "merge_dec_2 = concatenate([conv_enc_3, up_dec_2], axis = 3)\n",
    "conv_dec_2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(merge_dec_2)\n",
    "conv_dec_2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_dec_2)\n",
    "\n",
    "# Block decoder 3\n",
    "up_dec_3 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(UpSampling2D(size = (2,2))(conv_dec_2))\n",
    "merge_dec_3 = concatenate([conv_enc_2, up_dec_3], axis = 3)\n",
    "conv_dec_3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(merge_dec_3)\n",
    "conv_dec_3 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_dec_3)\n",
    "\n",
    "# Block decoder 4\n",
    "up_dec_4 = Conv2D(64, 2, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(UpSampling2D(size = (2,2))(conv_dec_3))\n",
    "merge_dec_4 = concatenate([conv_enc_1, up_dec_4], axis = 3)\n",
    "conv_dec_4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(merge_dec_4)\n",
    "conv_dec_4 = Conv2D(64, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_dec_4)\n",
    "conv_dec_4 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = initializer, kernel_regularizer=regularizers.l2(0.0001))(conv_dec_4)\n",
    "\n",
    "# -- Dencoder -- #\n",
    "\n",
    "output = Conv2D(1, 1, activation = 'sigmoid')(conv_dec_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zmdx4W31rTy5"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs = inputs, outputs = output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQrXnZcwrWZh"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss = 'binary_crossentropy',\n",
    "              metrics= ['accuracy', 'Precision', 'Recall'])\n",
    "#model.compile(optimizer=Adam(learning_rate=0.0001), loss = 'binary_crossentropy',\n",
    "              #metrics= ['accuracy'])\n",
    "# tf.keras.losses.binary_crossentropy, tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W1-QKj-HrYpp"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask: tf.Tensor) -> tf.Tensor:\n",
    "    \"\"\"Return a filter mask with the top 1 predicitons\n",
    "    only.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pred_mask : tf.Tensor\n",
    "        A [IMG_SIZE, IMG_SIZE, N_CLASS] tensor. For each pixel we have\n",
    "        N_CLASS values (vector) which represents the probability of the pixel\n",
    "        being these classes. Example: A pixel with the vector [0.0, 0.0, 1.0]\n",
    "        has been predicted class 2 with a probability of 100%.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tf.Tensor\n",
    "        A [IMG_SIZE, IMG_SIZE, 1] mask with top 1 predictions\n",
    "        for each pixels.\n",
    "    \"\"\"\n",
    "    # pred_mask -> [IMG_SIZE, SIZE, N_CLASS]\n",
    "    # 1 prediction for each class but we want the highest score only\n",
    "    # so we use argmax\n",
    "    pred_mask = tf.argmax(pred_mask, axis=-1) # Xing: here -1 may have the problem\n",
    "    # pred_mask becomes [IMG_SIZE, IMG_SIZE]\n",
    "    # but matplotlib needs [IMG_SIZE, IMG_SIZE, 1]\n",
    "    pred_mask = tf.expand_dims(pred_mask, axis=-1)\n",
    "    return pred_mask\n",
    "\n",
    "def show_predictions(dataset=None, num=1):\n",
    "    \"\"\"Show a sample prediction.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : [type], optional\n",
    "        [Input dataset, by default None\n",
    "    num : int, optional\n",
    "        Number of sample to show, by default 1\n",
    "    \"\"\"\n",
    "    if dataset:\n",
    "        for image, mask in dataset.take(num):\n",
    "            pred_mask = model.predict(image)\n",
    "            display_sample([image[0], true_mask, create_mask(pred_mask)])\n",
    "    else:\n",
    "        # The model is expecting a tensor of the size\n",
    "        # [BATCH_SIZE, IMG_SIZE, IMG_SIZE, 1]\n",
    "        # but sample_image[0] is [IMG_SIZE, IMG_SIZE, 1]\n",
    "        # and we want only 1 inference to be faster\n",
    "        # so we add an additional dimension [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "        one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "        # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "        inference = model.predict(one_img_batch)\n",
    "        # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "        pred_mask = inference\n",
    "        # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "        display_sample([sample_image[0], sample_mask[0],\n",
    "                        pred_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Vx52EVmrcwu"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset['train'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIzcS19orvon"
   },
   "source": [
    "Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sfu_WfNXrqeT"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "\n",
    "STEPS_PER_EPOCH = TRAINSET_SIZE // BATCH_SIZE\n",
    "VALIDATION_STEPS = VALSET_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMfH4eVQrzZI"
   },
   "outputs": [],
   "source": [
    "# sometimes it can be very interesting to run some batches on cpu\n",
    "# because the tracing is way better than on GPU\n",
    "# you will have more obvious error message\n",
    "# but in our case, it takes A LOT of time\n",
    "\n",
    "# On CPU\n",
    "# with tf.device(\"/cpu:0\"):\n",
    "#     model_history = model.fit(dataset['train'], epochs=EPOCHS,\n",
    "#                               steps_per_epoch=STEPS_PER_EPOCH,\n",
    "#                               validation_steps=VALIDATION_STEPS,\n",
    "#                               validation_data=dataset['val'])\n",
    "\n",
    "# On GPU\n",
    "model_history = model.fit(dataset['train'], epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=dataset['val'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2U_Ygr_9xCIF"
   },
   "outputs": [],
   "source": [
    "model.save(root+'1024_10_BF_5_training_50_saved_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h74P48piGkUG"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset['train'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fk7abws3IoDJ"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset['val'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3KBgW-Ya6kW"
   },
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#acc = history.history['acc']\n",
    "acc = model_history.history['accuracy']\n",
    "#val_acc = history.history['val_acc']\n",
    "val_acc = model_history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dGnRl6h08yc2"
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        clear_output(wait=True)\n",
    "        show_predictions()\n",
    "        print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CDgz3bvk81da"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
    "\n",
    "callbacks = [\n",
    "    # to show samples after each epoch\n",
    "    DisplayCallback(),\n",
    "    # to collect some useful metrics and visualize them in tensorboard\n",
    "    tensorboard_callback,\n",
    "    # if no accuracy improvements we can stop the training directly\n",
    "    tf.keras.callbacks.EarlyStopping(patience=20, verbose=1),\n",
    "    # to save checkpoints\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_model_unet.h5', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "]\n",
    "\n",
    "model = tf.keras.Model(inputs = inputs, outputs = output)\n",
    "\n",
    "# # here I'm using a new optimizer: https://arxiv.org/abs/1908.03265\n",
    "#optimizer=tfa.optimizers.RectifiedAdam(learning_rate=1e-3)\n",
    "\n",
    "#loss = tf.keras.losses.SparseCategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBL3005N85K9"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.0001), loss = tf.keras.losses.BinaryFocalCrossentropy(),\n",
    "              metrics= ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "#model.compile(optimizer=optimizer, loss = loss,\n",
    "                  #metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qd5id-379G0n"
   },
   "outputs": [],
   "source": [
    "model_history = model.fit(dataset['train'], epochs=EPOCHS,\n",
    "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                    validation_steps=VALIDATION_STEPS,\n",
    "                    validation_data=dataset['val'],\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WTmtfu_CG_6W"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset['train'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AJrz7de-S6z"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset['val'].take(1):\n",
    "    sample_image, sample_mask = image, mask\n",
    "\n",
    "show_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vk7Erfz4HfbH"
   },
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#acc = history.history['acc']\n",
    "acc = model_history.history['accuracy']\n",
    "#val_acc = history.history['val_acc']\n",
    "val_acc = model_history.history['val_accuracy']\n",
    "\n",
    "plt.plot(epochs, acc, 'y', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvIMzRiL2_CX"
   },
   "source": [
    "load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mXdkrmVe3BUB"
   },
   "outputs": [],
   "source": [
    "model_trained = tf.keras.models.load_model(root+'best_model_unet.h5')\n",
    "model_trained.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IevV3_mYZRe9"
   },
   "outputs": [],
   "source": [
    "train_dataset_test = tf.data.Dataset.list_files(dataset_path + training_data + \"*.jpg\", shuffle=False)\n",
    "train_dataset_test = train_dataset_test.map(parse_image)\n",
    "val_dataset_test = tf.data.Dataset.list_files(dataset_path + val_data + \"*.jpg\", shuffle=False)\n",
    "val_dataset_test = val_dataset_test.map(parse_image)\n",
    "dataset_test = {\"train\":train_dataset_test, \"val\": val_dataset_test}\n",
    "dataset_test['train'] = dataset_test['train'].map(load_image_test)\n",
    "dataset_test['train'] = dataset_test['train'].batch(1)\n",
    "dataset_test['val'] = dataset_test['val'].map(load_image_test)\n",
    "dataset_test['val'] = dataset_test['val'].batch(1)\n",
    "print(dataset_test['train'])\n",
    "print(dataset_test['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQE-0PVyABNA"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset_test['train'].take(5):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "    # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n",
    "    inference = model.predict(one_img_batch)\n",
    "    # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "    pred_mask = create_mask(inference)\n",
    "    # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "    display_sample([sample_image[0], sample_mask[0],\n",
    "                        pred_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f8HcQh3ZuVk"
   },
   "outputs": [],
   "source": [
    "for image, mask in dataset_test['val'].take(5):\n",
    "    sample_image, sample_mask = image, mask\n",
    "    one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "    # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n",
    "    inference = model.predict(one_img_batch)\n",
    "    # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "    pred_mask = create_mask(inference)\n",
    "    # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "    display_sample([sample_image[0], sample_mask[0],\n",
    "                        pred_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcsQRJ3kASxW"
   },
   "outputs": [],
   "source": [
    "for i in range(321):\n",
    "    for image, mask in dataset_test['train'].skip(i*10).take(1):\n",
    "      sample_image, sample_mask = image, mask\n",
    "      one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "      # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n",
    "      inference = model.predict(one_img_batch)\n",
    "      # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "      pred_mask = create_mask(inference)\n",
    "      # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "      #display_sample([sample_image[0], sample_mask[0],pred_mask[0]])\n",
    "\n",
    "      img_data=tf.keras.preprocessing.image.array_to_img(pred_mask[0])\n",
    "      plt.imsave(root+'results/.../'+str(i+1)+'.jpg',img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oJfYuUKE7Z-o"
   },
   "outputs": [],
   "source": [
    "for i in range(70):\n",
    "    for image, mask in dataset_test['val'].skip(i).take(1):\n",
    "      sample_image, sample_mask = image, mask\n",
    "      one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "      # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n",
    "      inference = model.predict(one_img_batch)\n",
    "      # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "      pred_mask = create_mask(inference)\n",
    "      # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "      #display_sample([sample_image[0], sample_mask[0],pred_mask[0]])\n",
    "\n",
    "      img_data=tf.keras.preprocessing.image.array_to_img(pred_mask[0])\n",
    "      plt.imsave(root+'results/'+str(i+1)+'.jpg',img_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h73gQDW60VCD"
   },
   "outputs": [],
   "source": [
    "def parse_image_test(img_path: str) -> dict:\n",
    "    \"\"\"Load an image and its annotation (mask) and returning\n",
    "    a dictionary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img_path : str\n",
    "        Image (not the mask) location.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary mapping an image and its annotation.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(img_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=1)\n",
    "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
    "\n",
    "    return {'image': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8XqdGco0aMx"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def normalize_test(input_image: tf.Tensor) -> tuple:\n",
    "    \"\"\"Rescale the pixel values of the images between 0.0 and 1.0\n",
    "    compared to [0,255] originally.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_image : tf.Tensor\n",
    "        Tensorflow tensor containing an image of size [SIZE,SIZE,3].\n",
    "    input_mask : tf.Tensor\n",
    "        Tensorflow tensor containing an annotation of size [SIZE,SIZE,1].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        Normalized image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    return input_image\n",
    "\n",
    "@tf.function\n",
    "def load_image_test_test(datapoint: dict) -> tuple:\n",
    "    \"\"\"Normalize and resize a test image and its annotation.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Since this is for the test set, we don't need to apply\n",
    "    any data augmentation technique.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    datapoint : dict\n",
    "        A dict containing an image and its annotation.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A modified image and its annotation.\n",
    "    \"\"\"\n",
    "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    input_image = normalize_test(input_image)\n",
    "\n",
    "    return input_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9OOPwNk10eGE"
   },
   "outputs": [],
   "source": [
    "test_data = \"test/\"\n",
    "\n",
    "test_dataset = tf.data.Dataset.list_files(dataset_path + test_data + \"*.jpg\", shuffle = False)\n",
    "test_dataset = test_dataset.map(parse_image_test)\n",
    "\n",
    "dataset_test = {\"test\":test_dataset}\n",
    "dataset_test['test'] = dataset_test['test'].map(load_image_test_test)\n",
    "dataset_test['test'] = dataset_test['test'].batch(1)\n",
    "print(dataset_test['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SFXk6ACl0ji8"
   },
   "outputs": [],
   "source": [
    "TEST_SIZE = len(glob(dataset_path + test_data + \"*.jpg\"))\n",
    "print(f\"The Training Dataset contains {TEST_SIZE} images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "ZasMq-AP0mKT"
   },
   "outputs": [],
   "source": [
    "for image in dataset_test['test'].take(3):\n",
    "    sample_image = image\n",
    "    one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "    # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n",
    "    inference = model_trained.predict(one_img_batch)\n",
    "    # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "    #pred_mask = create_mask(inference)\n",
    "    pred_mask = inference\n",
    "    # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "    display_sample([sample_image[0],pred_mask[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CwktjKLb0pPk"
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    for image in dataset_test['test'].skip(i).take(1):\n",
    "      sample_image = image\n",
    "      one_img_batch = sample_image[0][tf.newaxis, ...]\n",
    "      # one_img_batch -> [1, IMG_SIZE, IMG_SIZE, 3]\n",
    "      inference = model.predict(one_img_batch)\n",
    "      # inference -> [1, IMG_SIZE, IMG_SIZE, N_CLASS]\n",
    "      pred_mask = create_mask(inference)\n",
    "      # pred_mask -> [1, IMG_SIZE, IMG_SIZE, 1]\n",
    "      #display_sample([sample_image[0], sample_mask[0],pred_mask[0]])\n",
    "\n",
    "      img_data=tf.keras.preprocessing.image.array_to_img(pred_mask[0])\n",
    "      plt.imsave(root+'results/'+str(i+1)+'.jpg',img_data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuClass": "premium",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
